{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# os.chdir(\"D:\\\\UniSaarland\\\\Semester4\\\\DSP\\\\ProjectFiles\\\\Dark_Forest\\\\chunks\")\n",
    "\n",
    "# x, fs = librosa.load('chunk100.wav')\n",
    "# mfccs = librosa.feature.mfcc(x, sr=fs)\n",
    "# print(mfccs.shape)\n",
    "# mel_spec = librosa.feature.melspectrogram(x, sr=sr)\n",
    "# print(mel_spec.shape)\n",
    "# print(librosa.beat.beat_track(x, sr=sr), len(librosa.beat.beat_track(x, sr=sr)[1]))\n",
    "# S, phase = librosa.magphase(librosa.stft(x))\n",
    "# rms = librosa.feature.rmse(S=S)\n",
    "# np.mean(rms)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qs 1.1\n",
    " # We have used pydub for manipulating audio files. Command to install: 'pip install pydub'\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "import sys\n",
    "import os\n",
    "base_path = os.getcwd()\n",
    "\n",
    "genres =[\"Dark_Forest\",\"Hi_Tech\",\"Full_On\",\"Goa\"]  # Input names of the genre \n",
    "\n",
    "for genre in genres:\n",
    "    path = base_path+\"\\\\\"+ genre\n",
    "    if os.path.exists(path)== True:       \n",
    "        files = []\n",
    "        # r=root, d=directories, f = files\n",
    "        for r, d, f in os.walk(path):\n",
    "            for file in f:\n",
    "                if '.wav' in file:\n",
    "                    files.append(os.path.join(r, file))\n",
    "        i=0\n",
    "        dirc = path + \"\\\\chunks\"\n",
    "        if not os.path.exists(dirc):\n",
    "                os.makedirs(dirc)\n",
    "        for f in files:\n",
    "\n",
    "            print(f)\n",
    "            myaudio = AudioSegment.from_file(f , \"wav\") \n",
    "            chunk_length_ms = 30000 # pydub calculates in millisec\n",
    "            chunks = make_chunks(myaudio, chunk_length_ms) # Make chunks of 30 secs\n",
    "\n",
    "            for ind, chunk in enumerate(chunks):\n",
    "\n",
    "                chunk_name = dirc + \"\\chunk{0}.wav\".format(i)\n",
    "                print(chunk_name)\n",
    "                chunk.export(chunk_name, format=\"wav\")\n",
    "                i = i+1\n",
    "    else:\n",
    "        print(\"Directory doesn't exist!\") # If genre doesn't exist."
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retreiving features from  Dark_Forest\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "Retreiving features from  Hi_Tech\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "Retreiving features from  Full_On\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "Retreiving features from  Goa\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n"
     ]
    }
   ],
   "source": [
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qs 1.2.1 Standard features using existing libraries : 'pip install librosa'\n",
    "\n",
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "\n",
<<<<<<< HEAD
    "os.chdir(\"D:\\\\UniSaarland\\\\Semester4\\\\DSP\\\\ProjectFiles\")\n",
=======
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate tempo'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}'\n",
    "header += ' label'\n",
    "header = header.split()\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "os.chdir(\"D:\\\\UniSaarland\\\\Semester4\\\\DSP\\\\ProjectFiles\")\n",
=======
    "base_path = os.getcwd()\n",
    "\n",
    "\n",
    "genres =[\"Dark_Forest\",\"Hi_Tech\",\"Full_On\",\"Goa\"]  # Input name of the genre you want to preprocess\n",
    "\n",
    "\n",
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
    "file = open('data.csv', 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
<<<<<<< HEAD
    "genres =[\"Dark_Forest\",\"Hi_Tech\",\"Full_On\",\"Goa\"]\n",
    "\n",
    "for g in genres:\n",
    "    print(\"Retreiving features from \",g)\n",
    "    i = 0\n",
    "    for filename in os.listdir(f'.\\{g}\\chunks'):\n",
    "        print(i)\n",
    "        songname = f'.\\{g}\\chunks\\{filename}'\n",
    "        Filename = g+\"/\"+ filename\n",
    "        y, sr = librosa.load(songname)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        S, phase = librosa.magphase(librosa.stft(y))\n",
    "        rmse = librosa.feature.rmse(S=S)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        tempo,beat_vector = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        to_append = f'{Filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)} {tempo}'    \n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        to_append += f' {g}'\n",
    "        file = open('data.csv', 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())\n",
    "        i+=1\n",
    "        #y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "                \n",
=======
    "    \n",
    "for g in genres:\n",
    "    path = base_path+\"\\\\\"+ g\n",
    "    if os.path.exists(path)== True:\n",
    "        print(\"Retreiving features from \",g)\n",
    "        i = 0\n",
    "        for filename in os.listdir(f'.\\{g}\\chunks'):\n",
    "            print(i)\n",
    "            songname = f'.\\{g}\\chunks\\{filename}'\n",
    "            Filename = g+\"/\"+ filename\n",
    "            y, sr = librosa.load(songname)\n",
    "            chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "            spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "            spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "            rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "            zcr = librosa.feature.zero_crossing_rate(y)\n",
    "            S, phase = librosa.magphase(librosa.stft(y))\n",
    "            rmse = librosa.feature.rmse(S=S)\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "            tempo,beat_vector = librosa.beat.beat_track(y=y, sr=sr)\n",
    "            to_append = f'{Filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)} {tempo}'    \n",
    "            for e in mfcc:\n",
    "                to_append += f' {np.mean(e)}'\n",
    "            to_append += f' {g}'\n",
    "            file = open('data.csv', 'a', newline='')      # Qs 1.2.3 Storing the extracted features\n",
    "            with file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(to_append.split())\n",
    "            i+=1\n",
    "            #y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "    else:\n",
    "        print(\"Directory doesn't exist!\") # If genre doesn't exist.\n",
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
=======
   "execution_count": null,
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(df):\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    df_numeric = df.select_dtypes(include=numerics)\n",
    "    x = df_numeric.values #returns a numpy array\n",
    "    Standard_Scaler = preprocessing.StandardScaler()\n",
    "    x_scaled = Standard_Scaler.fit_transform(x)\n",
    "    df_scaled = pd.DataFrame(x_scaled)\n",
    "    df_scaled.insert(loc=0, column='filename', value=df.filename)\n",
    "    df_scaled[\"label\"] = df.label\n",
    "    df_scaled.columns = df.columns\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qs. 1.3 part 1 :\n",
    "# Normalizing features to zero mean and unit variance\n",
    "\n",
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
    "df = pd.read_csv(\"data.csv\",error_bad_lines=False)\n",
    "print(df.shape)\n",
    "print(df.tail())\n",
    "df_scaled = normalizer(df)\n",
<<<<<<< HEAD
    "df_scaled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.to_csv(\"df_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
=======
    "df_scaled.tail()\n",
    "df_scaled.to_csv(\"df_scaled.csv\")     # Storing the normalized features\n",
    "\n",
    "# Creating label encoder for the features\n",
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
    "genre_list = df_scaled.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(genre_list)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 27)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Dropping unneccesary columns\n",
    "df_scaled = df_scaled.drop(['filename'],axis=1)\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qs 1.3 part 2, Dividing data into training and testing set (75:25)\n",
    "\n",
    "\n",
    "df_scaled = df_scaled.drop(['filename'],axis=1)    # Dropping unneccesary columns\n",
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
    "X = np.array(df_scaled.iloc[:, :-1], dtype = float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
<<<<<<< HEAD
    "X_train.shape"
=======
    "print(X_train.shape)\n",
    "print(X_test.shape)"
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qs 2.1 k-NN classifier\n",
    "\n",
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.8333333333333334\n",
      "Recall Score: 0.8333333333333334\n",
      "Accuracy Score: 0.8333333333333334\n",
      "F1 Score: 0.8333333333333334\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
   "source": [
    "preds = knn.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,precision_score,recall_score,accuracy_score,f1_score,confusion_matrix\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds, average='micro')))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds, average='micro')))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds, average='micro')))\n",
    "    \n",
    "print_metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  6  2  1]\n",
      " [ 5 34  2  1]\n",
      " [ 0  3 95  1]\n",
      " [ 0  5 11 22]]\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
   "source": [
    "print(confusion_matrix(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 90,
=======
   "execution_count": null,
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        print(k)\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds, average='micro')\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    \n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "19\n",
      "21\n",
      "23\n",
      "25\n",
      "Best Value for k: 1\n",
      "F1-Score: 0.8783783783783784\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
   "source": [
    "find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.0"
=======
   "version": "3.6.10"
>>>>>>> 9d1b39ecd5f9de455da5c05e01ec58f87e3c53b0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
